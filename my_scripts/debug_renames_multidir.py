import subprocess
import os
import sys
import csv
import time
import argparse
import re
import ctypes
import shutil

# ================= é…ç½®åŒºåŸŸ =================

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DOC_DIR = os.path.join(PROJECT_ROOT, "my_docs")

TARGET_SCAN_DIRS = [
    os.path.join(PROJECT_ROOT, "src"),
    os.path.join(PROJECT_ROOT, "tools"),
]

REFACTOR_SCRIPT_PATH = os.path.join(SCRIPT_DIR, "content_refactor_batch.py")
SOURCE_CSV_PATH = os.path.join(DOC_DIR, "occt_renaming_map.csv")
WORK_CSV_PATH = os.path.join(DOC_DIR, "occt_renaming_map_new.csv")

BAD_ROWS_LOG = os.path.join(DOC_DIR, "bad_names.txt")
CHANGE_LOG = os.path.join(DOC_DIR, "fixed_names.txt")
GOOD_ROWS_LOG = os.path.join(DOC_DIR, "good_renames.txt")

SLN_PATH = os.path.normpath(os.path.join(PROJECT_ROOT, "..", "OCCTBUILD", "OCCT.sln"))

CMD_BUILD = ["msbuild", SLN_PATH, "/t:Build", "/p:Configuration=Release", "/maxCpuCount", "/p:StopOnFirstFailure=true"]
CMD_CLEAN = ["msbuild", SLN_PATH, "/t:Clean", "/p:Configuration=Release", "/maxCpuCount"]

CHUNK_SIZE = 50

# ========================================================

def disable_quick_edit():
    if os.name != 'nt': return
    try:
        kernel32 = ctypes.windll.kernel32
        hInput = kernel32.GetStdHandle(-10)
        mode = ctypes.c_ulong()
        if not kernel32.GetConsoleMode(hInput, ctypes.byref(mode)): return
        new_mode = mode.value & ~0x0040
        kernel32.SetConsoleMode(hInput, new_mode)
    except: pass

def init_work_csv():
    if not os.path.exists(WORK_CSV_PATH):
        shutil.copy2(SOURCE_CSV_PATH, WORK_CSV_PATH)

def count_csv_rows(filepath):
    with open(filepath, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        try: next(reader)
        except StopIteration: return 0
        return sum(1 for row in reader)

def kill_process_tree(pid):
    try: subprocess.run(["taskkill", "/F", "/T", "/PID", str(pid)], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
    except: pass

def nuke_build_processes():
    targets = ["cl.exe", "link.exe", "vctip.exe", "mspdbsrv.exe", "msbuild.exe"]
    for proc in targets:
        subprocess.run(["taskkill", "/F", "/IM", proc], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
    time.sleep(0.5)

def run_command(cmd, desc, stop_on_error=False):
    print(f"  [æ‰§è¡Œ] {desc}...", end="", flush=True)
    start_time = time.time()
    cmd_str = list(map(str, cmd))
    process = None
    captured_error_line = None
    error_type = "NONE"
    
    try:
        process = subprocess.Popen(
            cmd_str, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            shell=False, text=True, encoding='utf-8', errors='replace'
        )
    except Exception as e:
        print(f"\n  âŒ å¯åŠ¨å¤±è´¥: {e}")
        return False, None, "FATAL"

    error_detected = False
    
    while True:
        line = process.stdout.readline()
        if line:
            if stop_on_error:
                l_low = line.lower()
                if ": error" in l_low or "error c" in l_low or "fatal error" in l_low:
                    error_detected = True
                    captured_error_line = line.strip()
                    if "lnk" in l_low: error_type = "LINKER"
                    elif "fatal" in l_low: error_type = "FATAL"
                    else: error_type = "COMPILER"
                    print(f"\n\n{'!'*10} æ•è· {error_type} {'!'*10}\n{captured_error_line}\n{'!'*35}")
                    kill_process_tree(process.pid)
                    break
        elif process.poll() is not None:
            break
        else:
            time.sleep(0.05)

    if process and process.poll() is None:
        kill_process_tree(process.pid)
        process.wait()

    duration = time.time() - start_time
    success = (not error_detected) and (process.returncode == 0)
    
    if success: print(f" -> âœ… ({duration:.1f}s)")
    else: print(f" -> ğŸ›‘ ({duration:.1f}s)")
    
    return success, captured_error_line, error_type

def run_clean():
    print("\n  [æ¸…ç†] æ‰§è¡Œ Clean...")
    nuke_build_processes()
    subprocess.run(list(map(str, CMD_CLEAN)), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=False)

def reset_all_targets():
    for target in TARGET_SCAN_DIRS:
        if not os.path.exists(target): continue
        for i in range(3):
            try:
                subprocess.run(["git", "checkout", "HEAD", "--", target], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                subprocess.run(["git", "clean", "-fd", target], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                break 
            except:
                if i==2: nuke_build_processes()
                time.sleep(1)

def apply_refactoring_to_all(start, end):
    for target in TARGET_SCAN_DIRS:
        if not os.path.exists(target): continue
        cmd = [
            "python", REFACTOR_SCRIPT_PATH, target, WORK_CSV_PATH,
            "--start_row", str(start), "--end_row", str(end), "--run"
        ]
        success, _, _ = run_command(cmd, f"æ›¿æ¢ '{os.path.basename(target)}'", stop_on_error=False)
        if not success: return False
    return True

# --- æ™ºèƒ½è¾…åŠ© ---

def get_all_current_names():
    names = set()
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            n = row.get('Suggested_New_Name', '').strip()
            if n: names.add(n)
    return names

def generate_fix_candidates(original_name, current_new_name, used_names):
    candidates = []
    if '_' not in original_name:
        return [original_name] if original_name != current_new_name else []

    parts = original_name.split('_', 1)
    prefix = parts[0]
    suffix = parts[1]
    upper_count = sum(1 for c in suffix if c.isupper())
    
    cand_join = prefix + suffix
    
    base_cand_drop = suffix
    cand_drop = base_cand_drop
    counter = 1
    while cand_drop in used_names and cand_drop != current_new_name:
        cand_drop = f"{base_cand_drop}{counter}"
        counter += 1
    
    if upper_count <= 1:
        candidates.append(cand_join)
        candidates.append(cand_drop)
    else:
        candidates.append(cand_drop)
        candidates.append(cand_join)
    
    candidates.append(original_name)
    
    final_candidates = []
    for c in candidates:
        if c != current_new_name and c not in final_candidates:
            final_candidates.append(c)
    return final_candidates

def update_csv_row(row_num, new_name):
    rows = []
    header = []
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig', newline='') as f:
        reader = csv.reader(f)
        header = next(reader)
        rows = list(reader)
    idx = row_num - 1
    if 0 <= idx < len(rows):
        col_idx = header.index('Suggested_New_Name')
        rows[idx][col_idx] = new_name
        with open(WORK_CSV_PATH, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            writer.writerows(rows)

def get_row_info(row_num):
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader, start=1):
            if i == row_num:
                return row['Original_Class_Name'], row['Suggested_New_Name']
    return None, None

def log_fix(row_num, old_name, fix_name, original_name):
    msg = f"Row {row_num}: FIX [{old_name}] -> [{fix_name}] (Orig: {original_name})"
    print(f"\nâœ… {msg}")
    try:
        with open(CHANGE_LOG, "a", encoding="utf-8") as f:
            f.write(msg + "\n")
    except: pass

def get_csv_range_map(start, end):
    name_to_row = {}
    try:
        with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for i, row in enumerate(reader, start=1):
                if i < start: continue
                if i > end: break
                new_name = row.get('Suggested_New_Name', '').strip()
                if new_name: name_to_row[new_name] = i
    except: pass
    return name_to_row

def extract_word_at_index(text, index):
    if index >= len(text): index = len(text) - 1
    if index < 0: return ""
    if not re.match(r'\w', text[index]):
        while index > 0 and not re.match(r'\w', text[index]): index -= 1
    if not re.match(r'\w', text[index]): return ""
    start = index
    while start > 0 and re.match(r'\w', text[start-1]): start -= 1
    end = index
    while end < len(text)-1 and re.match(r'\w', text[end+1]): end += 1
    return text[start : end+1]

def get_smart_suspects(error_line, start, end):
    if not error_line: return []
    pattern = r"(?:^\d+>)?\s*(.*)\((\d+),(\d+)\)\s*:\s*error"
    match = re.search(pattern, error_line)
    if not match: return []

    file_path = match.group(1).strip()
    line_num = int(match.group(2))
    col_num = int(match.group(3))
    
    if not os.path.exists(file_path): return []

    line_content = ""
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
            if line_num <= len(lines):
                line_content = lines[line_num - 1]
    except: return []

    if not line_content: return []

    chunk_map = get_csv_range_map(start, end)
    
    token_tier1 = extract_word_at_index(line_content, col_num - 1)
    if token_tier1 and token_tier1 in chunk_map:
        print(f"  [æ™ºèƒ½åˆ†æ] ğŸ¯ å‘½ä¸­ä½ç½®: '{token_tier1}' (Row {chunk_map[token_tier1]})")
        return [chunk_map[token_tier1]]

    print(f"  [æ™ºèƒ½åˆ†æ] ç²¾ç¡®æœªå‘½ä¸­ (æå–: '{token_tier1}')ï¼Œè½¬å…¨è¡Œæ‰«æ...")
    tokens_in_line = set(re.findall(r"\w+", line_content))
    suspects = []
    for token in tokens_in_line:
        if token in chunk_map:
            suspects.append(chunk_map[token])
    
    suspects.sort()
    if suspects:
        print(f"  [æ™ºèƒ½åˆ†æ] è¡Œå†…å«Œç–‘äºº: {suspects}")
    
    return suspects

# --- æ ¸å¿ƒè‡ªåŠ¨ä¿®å¤ ---

def attempt_auto_fix_and_verify_block(suspect_row, block_start, block_end):
    """
    é’ˆå¯¹å«Œç–‘è¡Œè¿›è¡Œè‡ªåŠ¨ä¿®å¤ï¼Œå¹¶ä½¿ç”¨æ•´ä¸ª Block (block_start~block_end) è¿›è¡ŒéªŒè¯ã€‚
    è¿”å›: 
      0: ä¿®å¤æˆåŠŸï¼Œä¸” Block é€šè¿‡ã€‚
      1: ä¿®å¤æˆåŠŸ (Error è½¬ç§»)ï¼ŒBlock ä»æœ‰å…¶ä»–é”™è¯¯ã€‚
      -1: ä¿®å¤å¤±è´¥ã€‚
    """
    print(f"\nğŸ”§ [è‡ªåŠ¨ä¿®å¤] æ­£åœ¨å°è¯•ä¿®å¤ç¬¬ {suspect_row} è¡Œ (Scope: {block_start}-{block_end})...")
    
    orig_name, curr_new_name = get_row_info(suspect_row)
    if not orig_name: return -1

    used_names = get_all_current_names()
    candidates = generate_fix_candidates(orig_name, curr_new_name, used_names)
    
    print(f"  åŸå§‹: {orig_name} | å½“å‰: {curr_new_name}")
    print(f"  æ–¹æ¡ˆ: {candidates}")

    for cand in candidates:
        print(f"  ğŸ‘‰ å°è¯•æ–¹æ¡ˆ: {cand} ...")
        
        # 1. ä¿®æ”¹ CSV
        update_csv_row(suspect_row, cand)
        
        # 2. æ›¿æ¢ä»£ç  (æ³¨æ„ï¼šè¿™é‡Œæ›¿æ¢æ•´ä¸ª Block èŒƒå›´ï¼)
        reset_all_targets()
        if not apply_refactoring_to_all(block_start, block_end):
            continue
            
        # 3. ç¼–è¯‘éªŒè¯ (æ•´ä½“ç¼–è¯‘)
        success, error_line, error_type = run_command(CMD_BUILD, "éªŒè¯(æ•´ä½“)", stop_on_error=True)
        
        # 4. åˆ¤æ–­ç»“æœ
        if success:
            log_fix(suspect_row, curr_new_name, cand, orig_name)
            # å®Œç¾ï¼šä¸ä»…ä¿®å¤äº†å«Œç–‘äººï¼Œè€Œä¸”æ•´ä¸ª Block éƒ½é€šäº†
            return 0 
        else:
            # ç¼–è¯‘å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯ä¸æ˜¯åŒä¸€ä¸ªé”™è¯¯
            # æˆ‘ä»¬å†æ¬¡è°ƒç”¨ get_smart_suspects çœ‹çœ‹ç°åœ¨çš„æŠ¥é”™è¡Œæ˜¯è°
            new_suspects = get_smart_suspects(error_line, block_start, block_end)
            
            # å¦‚æœæŠ¥é”™è¡Œå·²ç»ä¸æ˜¯å½“å‰çš„ suspect_rowï¼Œè¯´æ˜ suspect_row ä¿®å¥½äº†ï¼
            # (æˆ–è€…å¦‚æœ new_suspects ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ link é”™è¯¯ï¼Œæˆ‘ä»¬å‡è®¾ä¿®å¥½äº†)
            if not new_suspects or suspect_row not in new_suspects:
                print(f"  âœ… æ–¹æ¡ˆ {cand} æœ‰æ•ˆï¼é”™è¯¯å·²è½¬ç§» (è‡³ {new_suspects})ã€‚")
                log_fix(suspect_row, curr_new_name, cand, orig_name)
                # ä¿ç•™ CSV ä¿®æ”¹ï¼Œä½†è¿”å› 1ï¼Œå‘Šè¯‰å¤–å±‚ç»§ç»­æ’æŸ¥å…¶ä»–é”™è¯¯
                return 1
            else:
                print(f"     âŒ æ–¹æ¡ˆ {cand} æ— æ•ˆï¼Œé”™è¯¯ä»åœ¨ç¬¬ {suspect_row} è¡Œã€‚")
                # ç»§ç»­å¾ªç¯ä¸‹ä¸€ä¸ª candidate

    # 5. å…¨éƒ¨å¤±è´¥
    print(f"  âŒ æ‰€æœ‰ä¿®å¤æ–¹æ¡ˆå‡å¤±è´¥ã€‚å›é€€åˆ°åŸå§‹åã€‚")
    update_csv_row(suspect_row, orig_name) # å›é€€ CSV
    return -1

def log_good_range(start, end):
    if start <= end:
        print(f"  >>> âœ… èŒƒå›´é€šè¿‡: {start}-{end}")
        with open(GOOD_ROWS_LOG, "a", encoding="utf-8") as f:
            f.write(f"{start}-{end}\n")

def log_bad_row(row_num, reason=""):
    print(f"\n>>> âš ï¸  é”å®šåè¡Œ: {row_num} {reason}")
    try:
        with open(BAD_ROWS_LOG, "a", encoding="utf-8") as f:
            f.write(f"{row_num}\n")
    except: pass

def check_range(start, end, last_build_success=True):
    if start > end: return

    print(f"\n--- æ£€æŸ¥èŒƒå›´: {start} åˆ° {end} (å…± {end - start + 1} è¡Œ) ---")
    
    if not apply_refactoring_to_all(start, end):
        reset_all_targets()
        return

    success, error_line, error_type = run_command(CMD_BUILD, "ç¼–è¯‘æ£€æŸ¥", stop_on_error=True)

    if not success and error_type in ["LINKER", "FATAL"]:
        if not last_build_success:
            print(f"  [ç­–ç•¥] é‡åˆ° {error_type} ä¸”é‡è¯•é˜¶æ®µ -> æ¸…ç†å¹¶é‡è¯•...")
            run_clean()
            success, error_line, error_type = run_command(CMD_BUILD, "é‡è¯•ç¼–è¯‘", stop_on_error=True)
        else:
            print(f"  [ç­–ç•¥] é‡åˆ° {error_type} ä½†ä¸Šæ¬¡æˆåŠŸ -> åˆ¤å®šä¸ºä»£ç é”™è¯¯")

    # æ™ºèƒ½å®šä½
    suspect_rows = []
    if not success and start != end:
        suspect_rows = get_smart_suspects(error_line, start, end)

    # æ­¤æ—¶æˆ‘ä»¬è¿˜æ²¡æœ‰ Reset ä»£ç ï¼Œæ­£å¥½ç”¨äºåˆ†æ (å…¶å® get_smart_suspects å·²ç»è¯»å®Œäº†)
    # ä½†ä¸ºäº†åç»­æµç¨‹ï¼ˆæ¯”å¦‚ attempt_auto_fix é‡Œä¼šå…ˆ reset å† applyï¼‰ï¼Œè¿™é‡Œå…ˆä¸æ€¥ç€ reset
    # ä¸è¿‡ä¸Šé¢çš„ check_range é€»è¾‘é‡Œæ˜¯å…ˆ reset çš„ã€‚
    # é‰´äº attempt_auto_fix å†…éƒ¨ä¼š resetï¼Œè¿™é‡Œæˆ‘ä»¬å¯ä»¥å…ˆ reset ä¿æŒä¸€è‡´æ€§
    reset_all_targets()

    if success:
        log_good_range(start, end)
        return

    # === å¤±è´¥å¤„ç† ===
    
    # 1. é”å®šå•è¡Œåè¡Œ -> å°è¯•ä¿®å¤
    if start == end:
        # å•è¡Œæ¨¡å¼ä¸‹ï¼Œblock_start = block_end = start
        res = attempt_auto_fix_and_verify_block(start, start, start)
        if res == 0: # æˆåŠŸä¸”é€šè¿‡
            log_good_range(start, end)
        elif res == -1: # ä¿®å¤å¤±è´¥
            log_bad_row(start, f"(æ— æ³•ä¿®å¤)")
        return

    # 2. æ™ºèƒ½å®šä½å‘½ä¸­ -> éªŒè¯å¹¶å°è¯•ä¿®å¤
    if suspect_rows:
        for s_row in suspect_rows:
            # è¿™é‡Œçš„å…³é”®æ˜¯ï¼šç”¨å½“å‰å¤§å— (start, end) æ¥éªŒè¯ s_row çš„ä¿®å¤
            res = attempt_auto_fix_and_verify_block(s_row, start, end)
            
            if res == 0:
                # å®Œç¾ï¼šä¿®å¥½äº† s_rowï¼Œè€Œä¸”æ•´ä¸ª start-end éƒ½é€šäº†ï¼
                log_good_range(start, end)
                return 
            
            elif res == 1:
                # è¿›æ­¥ï¼šä¿®å¥½äº† s_rowï¼Œä½† start-end è¿˜æœ‰åˆ«çš„é”™
                # s_row å·²ç»åœ¨ CSV é‡Œè¢«æ”¹å¥½äº†ã€‚
                # æˆ‘ä»¬å¯ä»¥ä¸ç”¨æ€¥ç€é€’å½’ï¼Œè€Œæ˜¯ç›´æ¥åœ¨å½“å‰å±‚çº§â€œç»§ç»­â€äºŒåˆ†ï¼Ÿ
                # æˆ–è€…æ›´ç¨³å¥çš„åšæ³•ï¼šæ—¢ç„¶ç¯å¢ƒå˜äº†ï¼ˆCSVå˜äº†ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥é€’å½’äºŒåˆ†
                # æ­¤æ—¶ s_row å·²ç»æ˜¯â€œå¥½äººâ€äº†ã€‚
                print("  [æ™ºèƒ½åˆ†æ] å«Œç–‘äººå·²ä¿®å¤ï¼Œç»§ç»­æ’æŸ¥å‰©ä½™é”™è¯¯...")
                # æ­¤æ—¶ä¸éœ€è¦ returnï¼Œè®©ä»£ç èµ°åˆ°ä¸‹é¢çš„äºŒåˆ†æ³•
                # äºŒåˆ†æ³•ä¼šå†æ¬¡ç¼–è¯‘ï¼Œè¿™è™½ç„¶å¤šäº†ä¸€æ¬¡ç¼–è¯‘ï¼Œä½†é€»è¾‘æœ€ç®€å•
                # ä¼˜åŒ–ï¼šå¦‚æœæˆ‘ä»¬ç¡®ä¿¡é”™è¯¯åœ¨å¦ä¸€åŠï¼Œå¯ä»¥ç›´æ¥è·³è¿‡ s_row
                # ä½†ä¸ºäº†ç®€å•ï¼Œç›´æ¥ fall through åˆ° B æ­¥éª¤å³å¯
                break 

            # res == -1: æ²¡ä¿®å¥½ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå«Œç–‘äºº

    # B. å›é€€äºŒåˆ†æ³•
    mid = (start + end) // 2
    check_range(start, mid, last_build_success=False)
    check_range(mid + 1, end, last_build_success=False)

def main():
    disable_quick_edit()
    parser = argparse.ArgumentParser()
    parser.add_argument("--start_row", type=int, default=1)
    args = parser.parse_args()

    if not os.path.exists(SOURCE_CSV_PATH): return
    init_work_csv()
    
    log_mode = "w" if args.start_row == 1 else "a"
    try:
        with open(CHANGE_LOG, log_mode, encoding="utf-8") as f:
            if log_mode == "w": f.write(f"Fix Log\n")
        with open(GOOD_ROWS_LOG, log_mode, encoding="utf-8") as f:
            if log_mode == "w": f.write(f"Good Ranges Log\n")
        with open(BAD_ROWS_LOG, log_mode, encoding="utf-8") as f:
            if log_mode == "w": f.write(f"Bad Rows Log\n")
    except: pass

    total_rows = count_csv_rows(WORK_CSV_PATH)
    current = args.start_row
    
    print(f"æ€»è¡Œæ•°: {total_rows} | èµ·å§‹: {current} | æ­¥é•¿: {CHUNK_SIZE}")
    
    while current <= total_rows:
        end = current + CHUNK_SIZE - 1
        if end > total_rows: end = total_rows
        check_range(current, end, last_build_success=True)
        current = end + 1

    print("\næ‰«æå®Œæˆï¼")

if __name__ == "__main__":
    main()