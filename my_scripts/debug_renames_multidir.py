import subprocess
import os
import sys
import csv
import time
import argparse
import re
import ctypes
import shutil

# ================= é…ç½®åŒºåŸŸ =================

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
DOC_DIR = os.path.join(PROJECT_ROOT, "my_docs")

TARGET_SCAN_DIRS = [
    os.path.join(PROJECT_ROOT, "src"),
    os.path.join(PROJECT_ROOT, "tools"),
]

REFACTOR_SCRIPT_PATH = os.path.join(SCRIPT_DIR, "content_refactor_batch.py")

# åŸå§‹ CSV (åªè¯»)
SOURCE_CSV_PATH = os.path.join(DOC_DIR, "occt_renaming_map.csv")
# å·¥ä½œ CSV (è¯»å†™ï¼Œè„šæœ¬å°†ä¿®æ”¹æ­¤æ–‡ä»¶)
WORK_CSV_PATH = os.path.join(DOC_DIR, "occt_renaming_map_new.csv")

# æ—¥å¿—
CHANGE_LOG = os.path.join(DOC_DIR, "bad_names.txt") # è®°å½•ä¿®å¤æ—¥å¿—
GOOD_ROWS_LOG = os.path.join(DOC_DIR, "good_renames.txt")

SLN_PATH = os.path.normpath(os.path.join(PROJECT_ROOT, "..", "OCCTBUILD", "OCCT.sln"))

CMD_BUILD = [
    "msbuild", SLN_PATH, "/t:Build", "/p:Configuration=Release", 
    "/maxCpuCount", "/p:StopOnFirstFailure=true"
]

CMD_CLEAN = [
    "msbuild", SLN_PATH, "/t:Clean", "/p:Configuration=Release", "/maxCpuCount"
]

CHUNK_SIZE = 50

# ========================================================

def disable_quick_edit():
    if os.name != 'nt': return
    try:
        kernel32 = ctypes.windll.kernel32
        hInput = kernel32.GetStdHandle(-10)
        mode = ctypes.c_ulong()
        if not kernel32.GetConsoleMode(hInput, ctypes.byref(mode)): return
        new_mode = mode.value & ~0x0040
        kernel32.SetConsoleMode(hInput, new_mode)
    except: pass

def init_work_csv():
    """åˆå§‹åŒ–å·¥ä½œCSVï¼šå¦‚æœä¸å­˜åœ¨åˆ™å¤åˆ¶æºæ–‡ä»¶"""
    if not os.path.exists(WORK_CSV_PATH):
        print(f"[åˆå§‹åŒ–] åˆ›å»ºå‰¯æœ¬: {os.path.basename(WORK_CSV_PATH)}")
        shutil.copy2(SOURCE_CSV_PATH, WORK_CSV_PATH)
    else:
        print(f"[åˆå§‹åŒ–] ä½¿ç”¨ç°æœ‰å‰¯æœ¬: {os.path.basename(WORK_CSV_PATH)}")

def count_csv_rows(filepath):
    with open(filepath, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        try: next(reader)
        except StopIteration: return 0
        return sum(1 for row in reader)

# --- è¿›ç¨‹ç®¡ç† ---
def kill_process_tree(pid):
    try:
        subprocess.run(["taskkill", "/F", "/T", "/PID", str(pid)], 
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
    except: pass

def nuke_build_processes():
    targets = ["cl.exe", "link.exe", "vctip.exe", "mspdbsrv.exe", "msbuild.exe"]
    for proc in targets:
        subprocess.run(["taskkill", "/F", "/IM", proc], 
                       stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False)
    time.sleep(0.5)

def run_command(cmd, desc, stop_on_error=False):
    print(f"  [æ‰§è¡Œ] {desc}...", end="", flush=True)
    start_time = time.time()
    cmd_str = list(map(str, cmd))
    process = None
    captured_error_line = None
    error_type = "NONE"
    
    try:
        process = subprocess.Popen(
            cmd_str, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            shell=False, text=True, encoding='utf-8', errors='replace'
        )
    except Exception as e:
        print(f"\n  âŒ å¯åŠ¨å¤±è´¥: {e}")
        return False, None, "FATAL"

    error_detected = False
    
    while True:
        line = process.stdout.readline()
        if line:
            if stop_on_error:
                l_low = line.lower()
                is_err = ": error" in l_low or "error c" in l_low or "fatal error" in l_low
                if is_err:
                    error_detected = True
                    captured_error_line = line.strip()
                    if "lnk" in l_low: error_type = "LINKER"
                    elif "fatal" in l_low: error_type = "FATAL"
                    else: error_type = "COMPILER"
                    print(f"\n\n{'!'*10} æ•è· {error_type} {'!'*10}\n{captured_error_line}\n{'!'*35}")
                    kill_process_tree(process.pid)
                    break
        elif process.poll() is not None:
            break
        else:
            time.sleep(0.05)

    if process and process.poll() is None:
        kill_process_tree(process.pid)
        process.wait()

    duration = time.time() - start_time
    success = (not error_detected) and (process.returncode == 0)
    
    if success: print(f" -> âœ… ({duration:.1f}s)")
    else: print(f" -> ğŸ›‘ ({duration:.1f}s)")
    
    return success, captured_error_line, error_type

def run_clean():
    print("\n  [æ¸…ç†] æ‰§è¡Œ Clean...")
    nuke_build_processes()
    subprocess.run(list(map(str, CMD_CLEAN)), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=False)

def reset_all_targets():
    """Git å›æ»š"""
    for target in TARGET_SCAN_DIRS:
        if not os.path.exists(target): continue
        for i in range(3):
            try:
                subprocess.run(["git", "checkout", "HEAD", "--", target], 
                               check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                subprocess.run(["git", "clean", "-fd", target], 
                               check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                break 
            except:
                if i==2: nuke_build_processes()
                time.sleep(1)

def apply_refactoring_to_all(start, end):
    for target in TARGET_SCAN_DIRS:
        if not os.path.exists(target): continue
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ WORK_CSV_PATH
        cmd = [
            "python", REFACTOR_SCRIPT_PATH, target, WORK_CSV_PATH,
            "--start_row", str(start), "--end_row", str(end), "--run"
        ]
        success, _, _ = run_command(cmd, f"æ›¿æ¢ '{os.path.basename(target)}'", stop_on_error=False)
        if not success: return False
    return True

# --- è‡ªåŠ¨ä¿®å¤é€»è¾‘ ---

def get_all_current_names():
    """è¯»å–å·¥ä½œCSVä¸­æ‰€æœ‰å·²å­˜åœ¨çš„ Suggested_New_Nameï¼Œç”¨äºé˜²é‡"""
    names = set()
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            n = row.get('Suggested_New_Name', '').strip()
            if n: names.add(n)
    return names

def generate_fix_candidates(original_name, current_new_name, used_names):
    """
    ç”Ÿæˆå€™é€‰åå­—åˆ—è¡¨ (ä¼˜å…ˆçº§ä»é«˜åˆ°ä½)
    è§„åˆ™ 1: æ¨¡å—_å•è¯ -> æ¨¡å—å•è¯ (å»ä¸‹åˆ’çº¿)
    è§„åˆ™ 2: æ¨¡å—_å¤šè¯ -> å¤šè¯ (å»æ¨¡å—)
    è§„åˆ™ 3: å›é€€ (Original)
    """
    candidates = []
    
    # å¦‚æœæ²¡æœ‰ä¸‹åˆ’çº¿ï¼Œç›´æ¥å›é€€
    if '_' not in original_name:
        return [original_name] if original_name != current_new_name else []

    parts = original_name.split('_', 1)
    prefix = parts[0]
    suffix = parts[1]

    # åˆ¤æ–­ suffix æ˜¯å•ä¸ªå•è¯è¿˜æ˜¯å¤šä¸ªå•è¯ (ç®€å•çš„ CamelCase åˆ¤å®š)
    # æ¯”å¦‚ "Solid" -> 1ä¸ªå¤§å†™; "ParamCursor" -> 2ä¸ªå¤§å†™
    upper_count = sum(1 for c in suffix if c.isupper())
    
    cand_join = prefix + suffix  # è§„åˆ™1: TopoDSSolid
    
    # è§„åˆ™2: ParamCursor (éœ€å¤„ç†å†²çª)
    base_cand_drop = suffix
    cand_drop = base_cand_drop
    counter = 1
    # ç¡®ä¿ä¸å’Œç°æœ‰çš„å†²çª (æ’é™¤æ‰è‡ªå·±å½“å‰çš„åå­—ï¼Œå› ä¸ºæˆ‘ä»¬è¦æ”¹å®ƒ)
    while cand_drop in used_names and cand_drop != current_new_name:
        cand_drop = f"{base_cand_drop}{counter}"
        counter += 1
    
    # å†³ç­–ä¼˜å…ˆçº§
    if upper_count <= 1:
        # å•å•è¯: ä¼˜å…ˆ TopoDSSolidï¼Œå…¶æ¬¡ Solidï¼Œæœ€åå›é€€
        candidates.append(cand_join)
        candidates.append(cand_drop)
    else:
        # å¤šå•è¯: ä¼˜å…ˆ ParamCursorï¼Œå…¶æ¬¡ IGESDataParamCursorï¼Œæœ€åå›é€€
        candidates.append(cand_drop)
        candidates.append(cand_join)
    
    candidates.append(original_name) # å…œåº•ï¼šå›é€€

    # è¿‡æ»¤æ‰å’Œå½“å‰å¤±è´¥åå­—ä¸€æ ·çš„ï¼Œé¿å…é‡å¤è¯•
    final_candidates = []
    for c in candidates:
        if c != current_new_name and c not in final_candidates:
            final_candidates.append(c)
            
    return final_candidates

def update_csv_row(row_num, new_name):
    """æ›´æ–° CSV æŒ‡å®šè¡Œçš„ Suggested_New_Name"""
    rows = []
    header = []
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig', newline='') as f:
        reader = csv.reader(f)
        header = next(reader)
        rows = list(reader)
    
    # row_num æ˜¯ä»1å¼€å§‹çš„æ•°æ®è¡Œå·
    idx = row_num - 1
    if 0 <= idx < len(rows):
        # å‡è®¾ Suggested_New_Name æ˜¯ç¬¬3åˆ— (ç´¢å¼•2)
        # æ ¹æ®ä½ çš„ map1.csv ç»“æ„: Original_Package, Original_Class_Name, Suggested_New_Name
        col_idx = header.index('Suggested_New_Name')
        old_val = rows[idx][col_idx]
        rows[idx][col_idx] = new_name
        
        with open(WORK_CSV_PATH, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            writer.writerows(rows)
        return old_val
    return None

def get_row_info(row_num):
    """è·å–æŒ‡å®šè¡Œçš„åŸå§‹ä¿¡æ¯"""
    with open(WORK_CSV_PATH, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for i, row in enumerate(reader, start=1):
            if i == row_num:
                return row['Original_Class_Name'], row['Suggested_New_Name']
    return None, None

def log_fix(row_num, old_name, fix_name, original_name):
    msg = f"Row {row_num}: FIX [{old_name}] -> [{fix_name}] (Orig: {original_name})"
    print(f"\nâœ… {msg}")
    with open(CHANGE_LOG, "a", encoding="utf-8") as f:
        f.write(msg + "\n")

def attempt_auto_fix(row_num):
    """
    å°è¯•è‡ªåŠ¨ä¿®å¤åè¡Œ
    """
    print(f"\nğŸ”§ [è‡ªåŠ¨ä¿®å¤] æ­£åœ¨å°è¯•ä¿®å¤ç¬¬ {row_num} è¡Œ...")
    
    orig_name, curr_new_name = get_row_info(row_num)
    if not orig_name: return False

    used_names = get_all_current_names()
    candidates = generate_fix_candidates(orig_name, curr_new_name, used_names)
    
    print(f"  åŸå§‹ç±»å: {orig_name}")
    print(f"  å½“å‰å¤±è´¥: {curr_new_name}")
    print(f"  ä¿®å¤æ–¹æ¡ˆ: {candidates}")

    for cand in candidates:
        print(f"  ğŸ‘‰ å°è¯•æ–¹æ¡ˆ: {cand} ...")
        
        # 1. ä¿®æ”¹ CSV
        update_csv_row(row_num, cand)
        
        # 2. æ›¿æ¢ä»£ç  (å•è¡Œ)
        reset_all_targets() # å…ˆæ¸…ç†
        if not apply_refactoring_to_all(row_num, row_num):
            continue
            
        # 3. ç¼–è¯‘éªŒè¯ (ä¸ Cleanï¼Œå› ä¸ºæ˜¯å•è¡ŒéªŒè¯ï¼Œä¸”å‰é¢å¯èƒ½cleanè¿‡äº†)
        success, _, _ = run_command(CMD_BUILD, "éªŒè¯ä¿®å¤", stop_on_error=True)
        
        if success:
            log_fix(row_num, curr_new_name, cand, orig_name)
            reset_all_targets() # æˆåŠŸåæ¸…ç†ï¼Œå‡†å¤‡ä¸‹ä¸€æ­¥
            return True
        else:
            print(f"     âŒ æ–¹æ¡ˆ {cand} å¤±è´¥ã€‚")
            reset_all_targets()

    # å¦‚æœæ‰€æœ‰æ–¹æ¡ˆéƒ½å¤±è´¥ï¼ˆåŒ…æ‹¬å›é€€ï¼‰ï¼Œé‚£çœŸæ˜¯æ²¡æ•‘äº†
    print(f"  âŒ æ‰€æœ‰ä¿®å¤æ–¹æ¡ˆå‡å¤±è´¥ (åŒ…æ‹¬å›é€€)ã€‚ä¿ç•™æœ€åä¸€æ¬¡å°è¯•ã€‚")
    # æ¢å¤ä¸ºå›é€€çŠ¶æ€(é€šå¸¸æ˜¯æœ€åä¸€ä¸ªcandidate)
    update_csv_row(row_num, orig_name)
    with open(CHANGE_LOG, "a", encoding="utf-8") as f:
        f.write(f"Row {row_num}: FAILED TO FIX. Reverted to {orig_name}\n")
    return False

# --- ä¸»æµç¨‹ ---

def log_good_range(start, end):
    if start <= end:
        print(f"  >>> âœ… èŒƒå›´é€šè¿‡: {start}-{end}")
        with open(GOOD_ROWS_LOG, "a", encoding="utf-8") as f:
            f.write(f"{start}-{end}\n")

def check_range(start, end, last_build_success=True):
    if start > end: return

    print(f"\n--- æ£€æŸ¥èŒƒå›´: {start} åˆ° {end} (å…± {end - start + 1} è¡Œ) ---")
    
    if not apply_refactoring_to_all(start, end):
        reset_all_targets()
        return

    success, error_line, error_type = run_command(CMD_BUILD, "ç¼–è¯‘æ£€æŸ¥", stop_on_error=True)

    if not success and error_type in ["LINKER", "FATAL"]:
        if not last_build_success:
            print(f"  [ç­–ç•¥] é‡åˆ° {error_type} ä¸”å¤„äºé‡è¯•é˜¶æ®µ -> æ¸…ç†å¹¶é‡è¯•...")
            run_clean()
            success, _, _ = run_command(CMD_BUILD, "é‡è¯•ç¼–è¯‘", stop_on_error=True)
        else:
            print(f"  [ç­–ç•¥] é‡åˆ° {error_type} ä½†ä¸Šæ¬¡æˆåŠŸ -> åˆ¤å®šä¸ºä»£ç é”™è¯¯")

    reset_all_targets()

    if success:
        log_good_range(start, end)
        return

    # === å¤±è´¥å¤„ç† ===
    
    # 1. é”å®šåˆ°å•è¡Œ -> å¯åŠ¨è‡ªåŠ¨ä¿®å¤
    if start == end:
        print(f"âš ï¸  é”å®šåè¡Œ: {start}")
        attempt_auto_fix(start)
        return

    # 2. å°è¯•æ™ºèƒ½å®šä½
    # æ³¨æ„ï¼šget_smart_suspects åœ¨è¿™é‡Œæ— æ³•ä½¿ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç» reset äº†ä»£ç ï¼Œ
    # è€Œä¸”æˆ‘ä»¬ä¸èƒ½ä¾èµ–æŠ¥é”™ä¿¡æ¯åæŸ¥ New Name (å› ä¸ºæˆ‘ä»¬è¦æ”¹çš„å°±æ˜¯å®ƒ)ã€‚
    # é‰´äºæˆ‘ä»¬è¦ä¿®æ”¹ CSVï¼Œä¸ºäº†é€»è¾‘ç®€å•ç¨³å¥ï¼Œå»ºè®®ç›´æ¥é€€åŒ–ä¸ºäºŒåˆ†æ³•ï¼Œ
    # ç›´åˆ°ç¼©å°åˆ°å•è¡Œï¼Œå†è¿›è¡Œ Auto Fixã€‚
    
    # å¦‚æœä½ æƒ³ä¿ç•™æ™ºèƒ½å®šä½ï¼Œéœ€è¦åƒä¹‹å‰é‚£æ ·åœ¨ reset ä¹‹å‰åšï¼Œ
    # ä½†è€ƒè™‘åˆ°ç°åœ¨è¦æ”¹ CSVï¼ŒäºŒåˆ†æ³•è™½ç„¶æ…¢ç‚¹ä½†é€»è¾‘æœ€å®‰å…¨ã€‚
    
    mid = (start + end) // 2
    check_range(start, mid, last_build_success=False)
    check_range(mid + 1, end, last_build_success=False)

def main():
    disable_quick_edit()
    parser = argparse.ArgumentParser()
    parser.add_argument("--start_row", type=int, default=1)
    args = parser.parse_args()

    if not os.path.exists(SOURCE_CSV_PATH):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æº CSV {SOURCE_CSV_PATH}")
        return

    # åˆå§‹åŒ–å·¥ä½œç¯å¢ƒ
    init_work_csv()
    
    log_mode = "w" if args.start_row == 1 else "a"
    try:
        with open(CHANGE_LOG, log_mode, encoding="utf-8") as f:
            if log_mode == "w": f.write(f"Fix Log\n")
        with open(GOOD_ROWS_LOG, log_mode, encoding="utf-8") as f:
            if log_mode == "w": f.write(f"Good Ranges Log\n")
    except: pass

    total_rows = count_csv_rows(WORK_CSV_PATH)
    current = args.start_row
    
    print(f"æ€»è¡Œæ•°: {total_rows} | èµ·å§‹: {current} | æ­¥é•¿: {CHUNK_SIZE}")
    
    while current <= total_rows:
        end = current + CHUNK_SIZE - 1
        if end > total_rows: end = total_rows
        check_range(current, end, last_build_success=True)
        current = end + 1

    print("\n" + "="*60)
    print("å¤„ç†å®Œæˆï¼")
    print(f"æ–° CSV: {os.path.abspath(WORK_CSV_PATH)}")
    print(f"ä¿®å¤æ—¥å¿—: {os.path.abspath(CHANGE_LOG)}")
    print("="*60)

if __name__ == "__main__":
    main()